#!/usr/bin/env python3
"""Analyze strategy probe outputs with stronger statistical/ops metrics.

This script is designed for reliability probes generated by:
  scripts/codex_final_recall_probe.sh

It computes:
- Wilson confidence intervals for strict/semantic pass rates
- Kaplan-Meier style survival tables (event = failure turn)
- Failure stage distributions
- Throughput and runtime interruption rates
- Token/cost summaries (optional USD conversion via explicit rates)
- Snapshot boundary drift proxies from event annotations
"""

from __future__ import annotations

import argparse
import csv
import math
from collections import Counter, defaultdict
from dataclasses import dataclass
from pathlib import Path
from statistics import mean, pstdev
from typing import Dict, Iterable, List, Optional, Tuple


def parse_bool(s: str) -> bool:
    return str(s).strip().lower() in {"1", "true", "yes", "y"}


def to_int(s: str, default: int = -1) -> int:
    try:
        return int(str(s).strip())
    except Exception:
        return default


def normalize_text(s: str) -> str:
    return "".join(ch for ch in (s or "") if ch.isalnum() or ch == "_")


def wilson_ci(success: int, total: int, z: float = 1.96) -> Tuple[float, float, float]:
    """Return (rate, lo, hi) in [0,1]."""
    if total <= 0:
        return (0.0, 0.0, 0.0)
    p = success / total
    denom = 1.0 + z * z / total
    center = (p + z * z / (2.0 * total)) / denom
    half = z * math.sqrt((p * (1.0 - p) / total) + (z * z / (4.0 * total * total))) / denom
    return (p, max(0.0, center - half), min(1.0, center + half))


def read_tsv(path: Path) -> List[dict]:
    rows: List[dict] = []
    with path.open("r", encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter="\t")
        for row in reader:
            rows.append(row)
    return rows


@dataclass(frozen=True)
class TrialKey:
    strategy: str
    turns: int
    trial: int


@dataclass
class TrialStats:
    strategy: str
    turns: int
    trial: int
    duration_sec: float
    strict_pass: bool
    semantic_pass: bool
    fail_stage: str
    fail_turn: int
    guard_triggered: bool


def find_strategy_dirs(data_root: Path) -> List[Path]:
    out: List[Path] = []
    for p in sorted(data_root.iterdir()):
        if p.is_dir() and (p / "results.tsv").exists() and (p / "per_turn.tsv").exists():
            out.append(p)
    return out


def km_table(max_turn: int, fail_turns: List[Optional[int]]) -> List[dict]:
    """Discrete Kaplan-Meier table.

    fail_turns:
      - int t for failure event at turn t
      - None for right-censored at max_turn
    """
    event_times = sorted({t for t in fail_turns if t is not None and 1 <= t <= max_turn})
    table: List[dict] = []
    surv = 1.0
    for t in event_times:
        n_risk = sum(1 for ft in fail_turns if (ft is None and max_turn >= t) or (ft is not None and ft >= t))
        d = sum(1 for ft in fail_turns if ft == t)
        if n_risk > 0:
            surv *= (1.0 - d / n_risk)
        table.append(
            {
                "turn": t,
                "n_at_risk": n_risk,
                "events": d,
                "survival": surv,
            }
        )
    return table


def write_tsv(path: Path, rows: List[dict], fieldnames: List[str]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, delimiter="\t", fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)


def main() -> int:
    ap = argparse.ArgumentParser(description="Analyze strategy probe outputs (v2).")
    ap.add_argument("--data-root", default="data/strategy100", help="Root directory containing strategy subdirs")
    ap.add_argument("--out-dir", default="reports/v2", help="Output directory for v2 analysis artifacts")
    ap.add_argument("--price-input-usd-per-1m", type=float, default=None)
    ap.add_argument("--price-cached-input-usd-per-1m", type=float, default=None)
    ap.add_argument("--price-output-usd-per-1m", type=float, default=None)
    args = ap.parse_args()

    data_root = Path(args.data_root).resolve()
    out_dir = Path(args.out_dir).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    strategy_dirs = find_strategy_dirs(data_root)
    if not strategy_dirs:
        raise SystemExit(f"No strategy directories found under: {data_root}")

    trial_rows: List[TrialStats] = []
    per_turn_by_trial: Dict[TrialKey, List[dict]] = defaultdict(list)
    per_turn_events: List[dict] = []

    for sdir in strategy_dirs:
        strategy = sdir.name
        results = read_tsv(sdir / "results.tsv")
        per_turn = read_tsv(sdir / "per_turn.tsv")
        for r in results:
            turns = to_int(r.get("turns", "0"), 0)
            trial = to_int(r.get("trial", "0"), 0)
            trial_rows.append(
                TrialStats(
                    strategy=strategy,
                    turns=turns,
                    trial=trial,
                    duration_sec=float(r.get("duration_sec", "0") or 0.0),
                    strict_pass=parse_bool(r.get("strict_pass", "")),
                    semantic_pass=parse_bool(r.get("semantic_pass", "")),
                    fail_stage=(r.get("fail_stage", "") or "none").strip(),
                    fail_turn=to_int(r.get("fail_turn", "0"), 0),
                    guard_triggered=parse_bool(r.get("guard_triggered", "")),
                )
            )
        for row in per_turn:
            turns = to_int(row.get("turns", "0"), 0)
            trial = to_int(row.get("trial", "0"), 0)
            key = TrialKey(strategy=strategy, turns=turns, trial=trial)
            per_turn_by_trial[key].append(row)
            ev = (row.get("event", "") or "").strip()
            if ev and ev != "none":
                per_turn_events.append(
                    {
                        "strategy": strategy,
                        "turns": turns,
                        "trial": trial,
                        "turn_index": to_int(row.get("turn_index", "0"), 0),
                        "event": ev,
                        "expected": row.get("expected", "") or "",
                        "actual": row.get("actual", "") or "",
                    }
                )

    summary_rows: List[dict] = []
    failure_rows: List[dict] = []
    cost_rows: List[dict] = []
    drift_rows: List[dict] = []

    grouped: Dict[Tuple[str, int], List[TrialStats]] = defaultdict(list)
    for tr in trial_rows:
        grouped[(tr.strategy, tr.turns)].append(tr)

    for (strategy, turns), rows in sorted(grouped.items()):
        n = len(rows)
        strict_n = sum(1 for r in rows if r.strict_pass)
        semantic_n = sum(1 for r in rows if r.semantic_pass)
        strict_rate, strict_lo, strict_hi = wilson_ci(strict_n, n)
        sem_rate, sem_lo, sem_hi = wilson_ci(semantic_n, n)
        durations = [r.duration_sec for r in rows]
        avg_dur = mean(durations) if durations else 0.0
        std_dur = pstdev(durations) if len(durations) > 1 else 0.0
        guard_n = sum(1 for r in rows if r.guard_triggered)

        turns_completed_counts: List[int] = []
        timeout_or_nonzero = 0
        timeout_or_nonzero_total = 0
        input_sums: List[int] = []
        cached_sums: List[int] = []
        output_sums: List[int] = []
        final_inputs: List[int] = []

        for tr in rows:
            key = TrialKey(strategy=strategy, turns=turns, trial=tr.trial)
            pt = per_turn_by_trial.get(key, [])
            completed_non_init = 0
            in_sum = 0
            c_sum = 0
            o_sum = 0
            for row in pt:
                turn_idx = to_int(row.get("turn_index", "0"), 0)
                completed = parse_bool(row.get("completed", ""))
                if turn_idx > 0 and completed:
                    completed_non_init += 1
                in_tok = to_int(row.get("input_tokens", "-1"), -1)
                c_tok = to_int(row.get("cached_input_tokens", "-1"), -1)
                o_tok = to_int(row.get("output_tokens", "-1"), -1)
                if in_tok >= 0:
                    in_sum += in_tok
                if c_tok >= 0:
                    c_sum += c_tok
                if o_tok >= 0:
                    o_sum += o_tok
                note = (row.get("note", "") or "").strip()
                if note in {"timeout", "nonzero_exit"}:
                    timeout_or_nonzero += 1
                timeout_or_nonzero_total += 1
            turns_completed_counts.append(completed_non_init)
            input_sums.append(in_sum)
            cached_sums.append(c_sum)
            output_sums.append(o_sum)
            finals = [to_int(r.get("input_tokens", "-1"), -1) for r in pt if parse_bool(r.get("is_final", ""))]
            finals = [x for x in finals if x >= 0]
            if finals:
                final_inputs.append(finals[-1])

        avg_turns_completed = mean(turns_completed_counts) if turns_completed_counts else 0.0
        avg_tps = (avg_turns_completed / avg_dur) if avg_dur > 0 else 0.0
        interruption_rate = (
            timeout_or_nonzero / timeout_or_nonzero_total if timeout_or_nonzero_total > 0 else 0.0
        )

        summary_rows.append(
            {
                "strategy": strategy,
                "turns": turns,
                "n": n,
                "strict_pass": strict_n,
                "strict_rate": f"{strict_rate:.6f}",
                "strict_ci95_lo": f"{strict_lo:.6f}",
                "strict_ci95_hi": f"{strict_hi:.6f}",
                "semantic_pass": semantic_n,
                "semantic_rate": f"{sem_rate:.6f}",
                "semantic_ci95_lo": f"{sem_lo:.6f}",
                "semantic_ci95_hi": f"{sem_hi:.6f}",
                "avg_duration_sec": f"{avg_dur:.2f}",
                "std_duration_sec": f"{std_dur:.2f}",
                "avg_turns_completed": f"{avg_turns_completed:.2f}",
                "avg_turns_per_sec": f"{avg_tps:.6f}",
                "guard_stop_count": guard_n,
                "interrupt_rate": f"{interruption_rate:.6f}",
                "avg_final_input_tokens": f"{(mean(final_inputs) if final_inputs else 0.0):.1f}",
            }
        )

        fail_counter = Counter(r.fail_stage for r in rows)
        for stage, cnt in sorted(fail_counter.items()):
            failure_rows.append(
                {
                    "strategy": strategy,
                    "turns": turns,
                    "fail_stage": stage,
                    "count": cnt,
                    "rate": f"{(cnt / n) if n else 0.0:.6f}",
                }
            )

        avg_input_sum = mean(input_sums) if input_sums else 0.0
        avg_cached_sum = mean(cached_sums) if cached_sums else 0.0
        avg_output_sum = mean(output_sums) if output_sums else 0.0

        usd = ""
        if (
            args.price_input_usd_per_1m is not None
            and args.price_cached_input_usd_per_1m is not None
            and args.price_output_usd_per_1m is not None
        ):
            usd_val = (
                (avg_input_sum / 1_000_000.0) * args.price_input_usd_per_1m
                + (avg_cached_sum / 1_000_000.0) * args.price_cached_input_usd_per_1m
                + (avg_output_sum / 1_000_000.0) * args.price_output_usd_per_1m
            )
            usd = f"{usd_val:.6f}"

        cost_rows.append(
            {
                "strategy": strategy,
                "turns": turns,
                "n": n,
                "avg_sum_input_tokens_per_run": f"{avg_input_sum:.1f}",
                "avg_sum_cached_input_tokens_per_run": f"{avg_cached_sum:.1f}",
                "avg_sum_output_tokens_per_run": f"{avg_output_sum:.1f}",
                "estimated_usd_per_run": usd,
            }
        )

        # Survival table
        max_turn = max(turns - 1, 1)
        fail_turns: List[Optional[int]] = []
        for tr in rows:
            if tr.fail_stage == "none":
                fail_turns.append(None)
            elif tr.fail_turn > 0:
                fail_turns.append(tr.fail_turn)
            else:
                fail_turns.append(1)
        surv = km_table(max_turn=max_turn, fail_turns=fail_turns)
        write_tsv(
            out_dir / f"survival_{strategy}_{turns}.tsv",
            surv,
            ["turn", "n_at_risk", "events", "survival"],
        )

    # Drift/event proxy metrics.
    events_by_key: Dict[Tuple[str, int], List[dict]] = defaultdict(list)
    for e in per_turn_events:
        events_by_key[(e["strategy"], int(e["turns"]))].append(e)

    for (strategy, turns), rows in sorted(grouped.items()):
        events = events_by_key.get((strategy, turns), [])
        n_events = len(events)
        mismatch = 0
        boundary = 0
        boundary_mismatch = 0
        churn = 0
        churn_mismatch = 0
        for e in events:
            expected = normalize_text(e["expected"])
            actual = normalize_text(e["actual"])
            is_match = expected == actual
            if not is_match:
                mismatch += 1
            tags = {x.strip() for x in str(e["event"]).split(";") if x.strip()}
            if "snapshot_reset" in tags:
                boundary += 1
                if not is_match:
                    boundary_mismatch += 1
            if "requirement_churn" in tags:
                churn += 1
                if not is_match:
                    churn_mismatch += 1
        drift_rows.append(
            {
                "strategy": strategy,
                "turns": turns,
                "n_event_turns": n_events,
                "event_turn_mismatch_rate": f"{(mismatch / n_events) if n_events else 0.0:.6f}",
                "snapshot_boundary_count": boundary,
                "snapshot_boundary_mismatch_rate": f"{(boundary_mismatch / boundary) if boundary else 0.0:.6f}",
                "churn_event_count": churn,
                "churn_event_mismatch_rate": f"{(churn_mismatch / churn) if churn else 0.0:.6f}",
            }
        )

    write_tsv(
        out_dir / "summary_v2.tsv",
        summary_rows,
        [
            "strategy",
            "turns",
            "n",
            "strict_pass",
            "strict_rate",
            "strict_ci95_lo",
            "strict_ci95_hi",
            "semantic_pass",
            "semantic_rate",
            "semantic_ci95_lo",
            "semantic_ci95_hi",
            "avg_duration_sec",
            "std_duration_sec",
            "avg_turns_completed",
            "avg_turns_per_sec",
            "guard_stop_count",
            "interrupt_rate",
            "avg_final_input_tokens",
        ],
    )
    write_tsv(
        out_dir / "failure_stages_v2.tsv",
        failure_rows,
        ["strategy", "turns", "fail_stage", "count", "rate"],
    )
    write_tsv(
        out_dir / "cost_throughput_v2.tsv",
        cost_rows,
        [
            "strategy",
            "turns",
            "n",
            "avg_sum_input_tokens_per_run",
            "avg_sum_cached_input_tokens_per_run",
            "avg_sum_output_tokens_per_run",
            "estimated_usd_per_run",
        ],
    )
    write_tsv(
        out_dir / "drift_event_proxies_v2.tsv",
        drift_rows,
        [
            "strategy",
            "turns",
            "n_event_turns",
            "event_turn_mismatch_rate",
            "snapshot_boundary_count",
            "snapshot_boundary_mismatch_rate",
            "churn_event_count",
            "churn_event_mismatch_rate",
        ],
    )

    # Human-readable markdown summary.
    md = out_dir / "analysis_summary_v2.md"
    with md.open("w", encoding="utf-8") as f:
        f.write("# Strategy Probe v2 Analysis Summary\n\n")
        f.write("- This output is generated from raw TSV logs.\n")
        f.write("- CI method: Wilson 95% interval.\n")
        f.write("- Survival: discrete Kaplan-Meier style table per strategy.\n\n")

        f.write("## Files\n")
        f.write("- `summary_v2.tsv`\n")
        f.write("- `failure_stages_v2.tsv`\n")
        f.write("- `cost_throughput_v2.tsv`\n")
        f.write("- `drift_event_proxies_v2.tsv`\n")
        for s in sorted({r["strategy"] for r in summary_rows}):
            turns_vals = sorted({int(r["turns"]) for r in summary_rows if r["strategy"] == s})
            for t in turns_vals:
                f.write(f"- `survival_{s}_{t}.tsv`\n")
        f.write("\n")

        f.write("## Notes\n")
        if (
            args.price_input_usd_per_1m is None
            or args.price_cached_input_usd_per_1m is None
            or args.price_output_usd_per_1m is None
        ):
            f.write("- USD conversion is omitted because explicit token prices were not supplied.\n")
        else:
            f.write(
                "- USD conversion was applied with explicit per-1M token rates provided via CLI args.\n"
            )
        f.write(
            "- Drift metrics are event-based proxies; they are not full semantic compression-loss estimates.\n"
        )

    print(f"[done] wrote v2 analysis artifacts under: {out_dir}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
